## Case 1: The Loan Approval AI 💸🏦

### What’s happening:

A bank uses an AI system to determine whether a customer qualifies for a loan. The AI reviews various data points, including credit score, income, employment history, and even social media activity, to predict whether a person is a good candidate for a loan.

### What’s problematic:

1. Unfair Scoring from Social Media:
This AI is looking at more than just financial data—it’s also scanning social media profiles. While this might seem like a good way to get a “holistic” view of the borrower, it introduces massive risks for privacy invasion. It could unfairly penalize people who post about struggles like unemployment or financial hardship, even though those posts don't reflect someone's creditworthiness.

2. Discriminatory Risk Assessment:
The AI could also be making decisions based on biased historical data. For instance, if the training data is skewed toward a particular demographic or region, the system could inadvertently favor certain groups over others, leading to discrimination against marginalized communities.

### improvement idea:

To fix this, we should restrict the AI from using non-financial data, like social media activity, when determining loan eligibility. Financial decisions should be based on objective, relevant data (income, credit score, debt-to-income ratio, etc.). Additionally, the training data should be regularly audited to ensure it doesn’t reflect discriminatory biases, and the AI's decision-making process should be made transparent to customers so they know exactly what factors are influencing their loan approval.

## Case 2: The Fitness Tracking AI 🏋️‍♀️📱
What’s happening:

A fitness app uses AI to track users' workouts and provide personalized training plans. It collects data from wearables (heart rate, steps, calories burned) and uses that to recommend workouts, track progress, and even offer nutritional advice based on the user’s fitness goals.


### What’s problematic:

1. Over-Simplification of Health Data:
While fitness trackers provide valuable insights, they often over-simplify complex health data. For example, the AI might recommend a high-intensity workout based on the user’s step count or heart rate, without factoring in individual health conditions, injuries, or mental health status. This could lead to overexertion, especially for people with underlying health issues like heart conditions or chronic fatigue.

2. Privacy Concerns and Data Security:
The app collects a vast amount of personal health data, which is highly sensitive. If this data isn’t stored securely or if users don’t have full control over how it’s shared, it could be exposed in a data breach or misused for marketing purposes. Many users are not fully aware of how much data is being collected or how it might be used outside of fitness recommendations.

### improvement idea:

To improve this system, the app could incorporate a health questionnaire that asks users about any medical conditions or injuries before generating a workout plan. This would ensure the AI provides recommendations that are actually safe and suitable for the user's unique health needs. For privacy, the app should also offer clear opt-in consent for data collection, with easy-to-understand explanations about how data is stored, used, and shared. Plus, the data should be encrypted and anonymized to protect user privacy.

